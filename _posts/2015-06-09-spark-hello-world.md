---
layout: post
title: "Spark Hello World"
description: "Spark intro part 2"
category: [bigdata]
tags: [big data, spark, hello world, recommendations]
---

<ol>
<li>First, you need to go to: 


[Spark Downloads page](http://spark.apache.org/downloads.html) and then select the particular Spark release you want to install and the package type (you could choose to build it from source for various Hadoop versions or select a version pre built for a specific Hadoop version)</li>
<li>Next, unzip the file and move it to a convenient folder like (/home/username/Programming) or anything you are comfortable with</li>
<li>Spark runs on both Windows and Unix perfectly well. The only requirement in either case is that you need to have Java installed and in your system path or set up the JAVA_HOME environment variable. I set up the JAVA_HOME environment variable. For doing this, you would need to open the file ~/.bash_profile (create it if it does not exist) and add an entry as follows:


export JAVA_HOME='/path/to/your/java/home'

</li>
</ol>

